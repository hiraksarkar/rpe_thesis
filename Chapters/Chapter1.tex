\renewcommand{\subsectionmark}[1]{%
  \ifsubsectioninheader
    \def\subsectiontitle{: #1}%
  \else
    \def\subsectiontitle{}%
  \fi}
\newif\ifsubsectioninheader
\def\subsectiontitle{}
\fancyhead[L]{\nouppercase{\rightmark\ifsubsectioninheader\subsectiontitle\fi}}
\fancyhead[R]{\thepage}

\chapter{Introduction}
%highlight how biological analysis is driven by 
The advent of high-throughput sequencing has revolutionized the analysis of biological data. The enormous scale of technological improvement has doubled the capacity of DNA sequencing each year \citep{reuter2015high}. Be it the study of cellular processes, response of cellular pathways to drugs or transcriptional regulation, many different types of biological analysis are now highly dependant on an established pipeline of creating the experimental set up, choosing a suitable sequencing technology (sometimes designing one if needed) and sequencing a series of samples. The volume of data and speed of sequencing have widened the scope of rigorous statistical study and led to the application of sophisticated techniques from data science, e.g. as described in \citep{schatz2015biological}, in order to extract biological insights from vast volumes of sequencing data. High throughput sequencing technology also enables scientists to design protocols to serve particular applications, for example RNA-seq \citep{mortazavi2008mapping} and Ribo-seq \citep{gerashchenko2012genome} are designed to measure the level of mRNA and the rate of translation, respectively. The frequency of protein binding is revealed by ChiP-seq \citep{zhang2008model} technology, and nascent RNA transcription rate is measured by GRO-seq and PRO-seq \citep{core2008nascent} assays. Furthermore, recent single-cell technologies have enabled the probing of transcript expression profiles at the level of individual cells, which reveals the heterogeneity of different types of cells (i.e. different tissue types etc.). 

In this report we will mostly focus on RNA-seq data. Given the availability and low cost of production, RNA-seq has become one of the standard ways to measure mRNA abundance in a cell \citep{butte2000discovering}. It has wide application including \denovo transcriptome assembly, estimating isoform expression, and identifying novel transcripts and measuring differential expression under various conditions, like response to changing stimuli~\citep{Stubben2014} and disease states~\citep{diseaseDGE}.  Given the vast quantity of data often gathered to address such questions, studying these problems also requires computationally efficient methods capable of scaling with the experimental size. A number of algorithms (\citet{langmead2009ultrafast}, \citet{tophat}, \citet{mortazavi2008mapping}) have been designed to analyse the sequencing data from RNA-seq experiments. \citet{mortazavi2008mapping} have developed a statistical framework for abundance estimation from RNA-seq experiment. \citet{langmead2009ultrafast}, \citet{tophat} have  designed general purpose alignment tools for detail downstream analysis like splice detection, isoform identification etc.

%\note{\textbf{I think a bit more detail is needed here.  What challgenges did these algorithms solve?  Why did you choose these as representative? etc.}}. 

The widely-used RNA-seq data analysis pipeline, consists of three steps, first the single-end or the paired-end reads from the experiment are mapped or aligned to the reference transcriptome/genome. In case the reference transcriptome/genome is not present, either a \denovo or reference-guided assembly step is required to get an estimate of the contigs (longer contiguous region of sequences developped from raw reads) present in the sample. In the second step, abundance estimation is performed. Often, this is done by counting the reads that overlap an annotated or assembled transcript or gene, but mounting evidence has demonstrated that resolving the multi-mapping ambiguities by performing inference in an appropriate probabilistic model \citep{pachter2011models}, yields considerably more accurate abundances. After these two steps, the resulting estimates, i.e. the quantification results are used to find out the expression levels of the genes.  As described above, the variables of interest in the experimental design are used in conjunction with the quantification results to perform differential expression analysis. This allows one to study how the abundance of genes or transcripts changes with experimental factors; for example, to study how certain genes are down-regulated or up-regulated in response to a particular drug. 

It is important to note that RNA-seq experiments generate ``stable'' \citep{Chen2015} fragments from spliced transcripts, which makes the alignment step inherently difficult. A gene can express multiple isoforms, even in a single experiment. Isoforms from the same gene often share exons, which enables the generation of read sequences that, when mapped to transcriptome would naturally align with multiple transcripts at the same time. In this thesis we would propose a novel way to represent the data from a sequencing experiment by encoding the mapping information in terms of {\it equivalence classes}. Furthermore we would show two application where this representation can take aid to design robust and accurate algorithms. 

The first application aims to refine the \denovo transcriptome assemblies, resulting from different the state-of-the-art assemblers. Specifically, we design a method to cluster \denovo assembled contigs into putative genes based on shared, expressed sequence.  This enables ``gene-level'' analysis of \denovo transcriptome assemblies, mitigating the effect of assembly errors on quantification and differential analysis.  Despite the advanced algorithms used in assemblers, the reconstructed transcriptome often contains several errors including hybrid assembly where there are contigs from several gene families are put together, chimeric contigs (the spurious assembly of different underlying transcripts into a a single contig), spurious insertions and local mis-assemblies \citep{transrate}. Though improved computational methods can reduce the prevalence of such errors, the data itself is often insufficient to guarantee deterministic recovery of all expressed transcripts.  The fractured and incomplete nature of such \denovo assemblies can confound downstream analysis. We have devised a method that exploits the above mentioned equivalence classes to pose and solve a graph clustering problem, which allows us to cluster even fractured transcriptome assemblies into clusters representing putative genes. We implemented this concept as a tool \rapclust, which is explained in Chapter 2.

The second application we explore the idea of equivalence classes to derive a succinct representation of raw read sequences. From studying the mapping pattern of RNA-seq data, we have observed that a substantial fraction of reads map to a limited number of transcripts which are highly expressed in that sample. Even when multimapping is prevalent, reads tend to multimap between transcripts in consistent and repeated patterns, which can be exploited for the purposes of efficient compression. Transcripts are often covered by multi-mapping reads in a series of discontinuous region of highly abundant sequences, characterized as \islands. We design a semi-reference-based compression tool where \islands, which represent the relevant extracts from the reference, becomes a part of the encoding and later used to decode the compressed reads.
Our tool \quark, using \rapmap to map raw RNA-seq reads to the reference transcriptome. Chapter 3 is devoted for detail explanation of \quark. 

% more about equivalence classes




%For example,~\citet{corset} argue that the large number of contigs that often result from \denovo transcriptome assembly can greatly reduce the statistical power of differential expression analysis.      


\section{Alignment and Mapping}
Ambiguous mapping has made the problem of quantification extremely challenging for RNA-seq data. \citet{salzman2011statistical} and \citet{pachter2011models} described useful models that can be used for quantification under multi-mapping. \citet{sailfish} discovered the fact that a costly alignment is not really required for the purpose of quantification, related ideas that bypassed traditional notions of alignment were later used by \citet{kallisto} and \citet{salmon}. On the other hand \citet{rapmap},  proved that a stand alone exclusive mapper can be useful for multiple applications as demonstrated in Chapter 2 and Chapter 3. What all the aforementioned work share is the concept of putting similar reads together. Before we discuss the evolution of putting similar reads together in equivalence classes, let us formally define the mathematical framework of the sequence mapping/alignment paradigm. 

\subsection{Mathematical framework for nucleotide sequence matching}
Given a set of characters from an alphabet $\Sigma$, of size $|\Sigma|$, and two non-empty sequences $x,y \in \Sigma^+$, an alignment can be defined as a function $f : x \rightarrow \Sigma \cup \{-\}$, where, $``-"$ is a character to denote a gap has been introduced. Often there is a cost metric $d$ associated with the function $f$, defined as follows, 
\begin{align}
    d(x_i,y_j) &= 0,\text{ if } f(x_i) = y_j \\
    &= \text{mismatch penalty},\text{ if } f(x_i) \neq y_j \\
    &= \text{gap penalty},\text{ if } f(x_i) = ``-" 
\end{align}



The goal of most alignment algorithms \citep{Li2010} is to find alignments between the query and reference sequence to minimize the penalty of alignment, or to report if no alignment below a given penalty threshold exists. 

From the early days of bioinformatics, considerable effort \citep{Smith1981},\citep{Langmead2010},\citep{Li2008},\citep{Li2009}  has been put to develop efficient, fast and accurate sequence aligners.  

For mapping, as mentioned above, the mismatch or gaps are not considered as part of matching function. It is shown by \citet{salmon}, \citet{kallisto} and \citet{rapmap} that for certain downstream analyses, it is adequate to find out the location where read maps and the target sequence rather than full alignment information. These approaches often define mapping on the basis of a threshold. It is to be noted, although \citet{sailfish} did not map the reads explicitly, it introduced the concept eliding the traditional read alignment step which, in turn, inspired the batch of mapper dependent tools that followed it.

\subsection{Mapping in framework of RNA-seq}
Although at conceptual level RNA-seq mapping is not very different from alignment described above, but it would be worthwhile to define if formally as, these definitions are used through out the rest of the report.

Given a set of $M$ transcripts (or transcriptome) $T = \{t_1,t_2,\ldots,t_M\}$, where $t_i$ represents the nucleotide sequence for transcript $i$, likewise  $R = \{r_1,r_2,\ldots,r_N\}$ denotes a set of read sequences (might be paired end or single end), we can define the task of mapping as finding a function $\mathcal{Q} : {T,R} \rightarrow \mathcal{P}$, where $\mathcal{P}$ is a set of tuples. $\mathcal{P} = \{(r_i,t_j,p_k) : r_i \in R, t_i \in T, p_k \in \mathbb{N}\}$. The presence of tuple $(r_i,t_j,p_k)$ represents the fact that read $r_i$ is mapped to transcript $t_j$ at position $p_k$. An additional relevant information for reads with stranded protocol (sense or anti-sense) is the orientation of mapping, which often added as a flag with the mapping results
The concept of equivalence classes, described in this report, are derived from $\mathcal{P}$.  

\section{Equivalence class as a concept}

The concept of grouping similar reads together is not new. It has been mentioned several times in the literature either for improving multiple alignments or for constructing a consensus. Although it is hard to provide a full chronological account for the exact mention of such a concept in the context of computational biology, we would try to cover some important papers that implicitly made use of such a concept. 

\citet{pop2004} proposed the concept of grouping highly overlapping reads in the context of genome assembly. It has been shown that this kind of grouping greatly improve the performance and efficiency of iterative optimization algorithms such as EM \citep{sailfish}. \citet{salzman2011statistical} proposed factorization on likelihood function to speed up the quantification process. This speed up is realized by collapsing same splice junctions and exons into equivalence classes.
\citet{Salmela2011} considered the idea of grouping reads together on the basis of k-mer matches. \citet{isoem} have used similar equivalence classes over fragments and shown significant reduce in memory usage, increasing the speed of inference algorithm. Equivalence classes were defined on the pair of fragments that align to the same set of transcripts and whose compatibility weights with respect to the set of transcripts are proportional.  

% \note{Here is some text I wrote for the salmon manuscript describing the history of equivalence classes in RNA-seq explicitly.  Obviously, you shouldn't take any of this verbatim, but feel free to use it for inspiration below if you want to add anything
% Collapsing fragments into equivalence classes is a
% well-established idea in the transcript quantification literature,
% and numerous different notions of equivalence classes have
% been previously introduced, and shown to greatly reduce the time
% required to perform iterative optimization such as that described
% in XXX. For example, Salzman et al. (2011) first introduced the notion of factorizing the likelihood function to speed up inference by collapsing fragments that align to the same exons or exon junctions (as determined by a provided annotation) into equivalence classes. Simlarly, Nicolae et al. (2011) used equivalence classes over fragments to reduce memory usage and speed upinference --- they define as equivalent any pair of fragments that align to the same set of transcripts and whose compatibility weights (i.e. conditional probabilities) with respect to those transcripts are proportional.  Patro et al.(2014) define equivalence classes over k-mers, treating as equivalent any k-mers that appear in the same set of transcripts at the same frequency, and use this factorization of the likelihood function to speed up optimization.  Bray et al. (2015) define equivalence classes over fragments, and define as equivalent any fragments that pseuoalign to the same set of transcripts --- this is similar to the notion adopted by Nicolae et al., except that no restriction is placed on the proportionality of compatibilityweights (since these are not computed).}




 As mentioned before \citep{sailfish} elaborated the idea to implement a k-mer based grouping of the reads. Recent methods \citep{kallisto},\citep{salmon},\citep{rapmap} have implemented it more explicitly. 

Given this back ground on progression of development we can now formally define the idea of grouping reads and equivalence classes in the context of the current report.  

\subsection{Definition of equivalence classes}
\label{subsec:gen_equiv_classes}
We define an equivalence relation over reads, based on the set of transcripts to which they map.  The set of reads related under this definition constitutes a read equivalence class. Let $\mathcal{M}\left(r_i\right)$ be the set of transcripts to which read $r_i$ maps, and let $\mathcal{M}\left(r_j\right)$ be the set of transcripts to which read $r_j$ maps.  We say that $r_i \sim r_j$ if and only if $\mathcal{M}\left(r_i\right) = \mathcal{M}\left(r_j\right)$. Consequently, a read equivalence class is a set of reads such that, for every pair $r_i$ and $r_j$ in the class, $r_i \sim r_j$. An equivalence class can be uniquely labeled based on the set of transcripts to which the reads contained in this class map.  We define the label of equivalence class $\eqclass{r_i} = \{ r_j \in R \mid r_j \sim r_i\}$ as $\eqlabel{\eqclass{r_i}}$.  It is important to remember that, though the label consists of transcript names, the equivalence relation itself is defined over sequenced fragments and not transcripts. Finally, in addition to a label, we denote the count of each equivalence class $C_i$ by $\eqcount{C_i}$; this is simply the number of equivalent fragments in $C_i$.


Another interesting way to look at function $\mathcal{M}$ is to consider it's output on a particular set of fragments as a binary matrix. As shown in (1.4) a binary matrix can represent the read to transcript mapping function. After rearranging the rows we obtain $\mathcal{M'}$. Three equivalence classes can be extracted from here, $\{t_1,t_2,t_4\}, \{t_1\}$ and $\{t_1,t_4\}$. 

\begin{align}
  \mathcal{M}=\kbordermatrix{%
      & t_1 & t_2 & t_3 & t_4 \\
    r_1 & 1 & 1 & 0 & 1 \\
    r_2 & 1 & 0 & 0 & 0 \\
    r_3 & 1 & 1 & 0 & 1 \\
    r_4 & 1 & 0 & 0 & 0 \\
    r_5 & 1 & 1 & 0 & 1 \\
    r_6 & 1 & 0 & 0 & 1 \\
  }  &&
  \mathcal{M'}=\kbordermatrix{%
      & t_1 & t_2 & t_3 & t_4 \\
    r_1 & 1 & 1 & 0 & 1 \\
    r_3 & 1 & 1 & 0 & 1 \\
    r_5 & 1 & 1 & 0 & 1 \\
    \hline
    r_2 & 1 & 0 & 0 & 0 \\
    r_4 & 1 & 0 & 0 & 0 \\
    \hline
    r_6 & 1 & 0 & 0 & 1 \\
  }
\end{align}
 
 As we will see in subsequent chapters, this idea can be utilized to infer biological insights and represent the sequences in a succinct manner. 


%\section{}

